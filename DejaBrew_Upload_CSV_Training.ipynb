{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ DejaBrew Forecasting - Upload Your Own CSV\n",
        "\n",
        "**Use this notebook if you already have `coffee_shop_sales.csv` downloaded locally**\n",
        "\n",
        "This simplified notebook allows you to:\n",
        "1. ‚úÖ Upload your local CSV file\n",
        "2. ‚úÖ Train Gradient Boosting models\n",
        "3. ‚úÖ Evaluate accuracy metrics\n",
        "4. ‚úÖ Download trained models for your DejaBrew system\n",
        "\n",
        "**No Kaggle API setup needed!**"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn pandas numpy joblib matplotlib seaborn -q\n",
        "print(\"‚úì Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì§ Step 2: Upload Your CSV File\n",
        "\n",
        "Click the **Choose Files** button below and select your `coffee_shop_sales.csv` file."
      ],
      "metadata": {
        "id": "upload"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì§ Please upload your coffee_shop_sales.csv file:\")\n",
        "print(\"   (Click 'Choose Files' below)\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check what was uploaded\n",
        "uploaded_files = list(uploaded.keys())\n",
        "print(f\"\\n‚úÖ Uploaded: {uploaded_files}\")\n",
        "\n",
        "# Find the CSV file\n",
        "csv_file = None\n",
        "for filename in uploaded_files:\n",
        "    if filename.endswith('.csv'):\n",
        "        csv_file = filename\n",
        "        break\n",
        "\n",
        "if csv_file:\n",
        "    print(f\"‚úì Found CSV file: {csv_file}\")\n",
        "    print(f\"  File size: {len(uploaded[csv_file]):,} bytes\")\n",
        "    \n",
        "    # Rename to standard name if needed\n",
        "    if csv_file != 'coffee_shop_sales.csv':\n",
        "        os.rename(csv_file, 'coffee_shop_sales.csv')\n",
        "        csv_file = 'coffee_shop_sales.csv'\n",
        "        print(f\"  Renamed to: coffee_shop_sales.csv\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Ready to proceed!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Error: No CSV file found in upload.\")\n",
        "    print(\"   Please upload a .csv file and try again.\")"
      ],
      "metadata": {
        "id": "upload_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Step 3: Load and Validate Data"
      ],
      "metadata": {
        "id": "load"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading coffee_shop_sales.csv...\\n\")\n",
        "\n",
        "try:\n",
        "    # Load CSV\n",
        "    df_raw = pd.read_csv('coffee_shop_sales.csv')\n",
        "    \n",
        "    print(f\"‚úì CSV loaded successfully!\")\n",
        "    print(f\"  Shape: {df_raw.shape}\")\n",
        "    print(f\"\\nColumns found:\")\n",
        "    for i, col in enumerate(df_raw.columns, 1):\n",
        "        print(f\"  {i}. {col}\")\n",
        "    \n",
        "    # Check for required columns\n",
        "    required_cols = ['transaction_date', 'product_detail', 'transaction_qty']\n",
        "    missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"\\n‚ö†Ô∏è Warning: Missing expected columns: {missing_cols}\")\n",
        "        print(\"\\nPlease verify your CSV has these columns:\")\n",
        "        print(\"  - transaction_date (or similar date column)\")\n",
        "        print(\"  - product_detail (or similar product name column)\")\n",
        "        print(\"  - transaction_qty (or similar quantity column)\")\n",
        "        print(\"\\nYou may need to adjust the column_mapping in the next step.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All required columns found!\")\n",
        "    \n",
        "    print(\"\\nFirst 3 rows:\")\n",
        "    display(df_raw.head(3))\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: coffee_shop_sales.csv not found.\")\n",
        "    print(\"   Please run Step 2 to upload your CSV file.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading CSV: {e}\")"
      ],
      "metadata": {
        "id": "load_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 4: Preprocess Data\n",
        "\n",
        "**Important**: If your columns have different names, update the `column_mapping` dictionary below."
      ],
      "metadata": {
        "id": "preprocess"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Column mapping - UPDATE THIS if your CSV has different column names\n",
        "column_mapping = {\n",
        "    'transaction_date': 'date',      # Date column\n",
        "    'product_detail': 'product',     # Product name column\n",
        "    'transaction_qty': 'quantity'    # Quantity column\n",
        "}\n",
        "\n",
        "# Alternative examples (uncomment if needed):\n",
        "# column_mapping = {'Date': 'date', 'Product': 'product', 'Qty': 'quantity'}\n",
        "# column_mapping = {'date': 'date', 'item': 'product', 'qty': 'quantity'}\n",
        "\n",
        "print(\"Preprocessing data...\\n\")\n",
        "\n",
        "try:\n",
        "    # Apply column mapping\n",
        "    df = df_raw.rename(columns=column_mapping)\n",
        "    \n",
        "    # Keep only needed columns\n",
        "    df = df[['date', 'product', 'quantity']].copy()\n",
        "    \n",
        "    # Convert date to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    \n",
        "    # Remove invalid quantities\n",
        "    df = df[df['quantity'] > 0]\n",
        "    \n",
        "    # Clean product names\n",
        "    df['product'] = df['product'].str.strip()\n",
        "    \n",
        "    # Aggregate daily sales per product\n",
        "    df_daily = df.groupby(['date', 'product'])['quantity'].sum().reset_index()\n",
        "    \n",
        "    print(\"‚úÖ Data preprocessed successfully!\")\n",
        "    print(f\"\\nDataset Summary:\")\n",
        "    print(f\"  Total transactions: {len(df):,}\")\n",
        "    print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
        "    print(f\"  Total days: {(df['date'].max() - df['date'].min()).days}\")\n",
        "    print(f\"  Unique products: {df['product'].nunique()}\")\n",
        "    \n",
        "    print(f\"\\nTop 10 products by total sales:\")\n",
        "    top_products = df.groupby('product')['quantity'].sum().sort_values(ascending=False).head(10)\n",
        "    for i, (product, qty) in enumerate(top_products.items(), 1):\n",
        "        print(f\"  {i}. {product}: {int(qty):,} units\")\n",
        "    \n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå Error: Column not found: {e}\")\n",
        "    print(\"\\nPlease update the column_mapping dictionary above with the correct column names.\")\n",
        "    print(f\"Available columns: {df_raw.columns.tolist()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error preprocessing data: {e}\")"
      ],
      "metadata": {
        "id": "preprocess_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Step 5: Train Models\n",
        "\n",
        "This will train Gradient Boosting models for the top 30 products."
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('trained_models', exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"  TRAINING GRADIENT BOOSTING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Pivot to daily format\n",
        "daily = df_daily.pivot_table(index='date', columns='product', values='quantity', aggfunc='sum').fillna(0)\n",
        "\n",
        "# Create date features\n",
        "def create_date_features(df_index):\n",
        "    features = pd.DataFrame(index=df_index)\n",
        "    features['day_of_week'] = features.index.dayofweek\n",
        "    features['month'] = features.index.month\n",
        "    features['day_of_year'] = features.index.dayofyear\n",
        "    features['year'] = features.index.year\n",
        "    return features\n",
        "\n",
        "X = create_date_features(daily.index)\n",
        "\n",
        "# Get top 30 products\n",
        "TOP_N = 30\n",
        "all_sales = daily.sum().sort_values(ascending=False)\n",
        "top_products = all_sales.head(TOP_N).index.tolist()\n",
        "\n",
        "print(f\"\\nTraining models for top {len(top_products)} products...\\n\")\n",
        "\n",
        "trained_list = []\n",
        "metrics_summary = []\n",
        "\n",
        "for i, product in enumerate(top_products, 1):\n",
        "    if product not in daily.columns:\n",
        "        continue\n",
        "    \n",
        "    y = daily[product]\n",
        "    \n",
        "    # Check data\n",
        "    if len(X) < 30:\n",
        "        print(f\"[{i}/{len(top_products)}] ‚äò {product} - Not enough data\")\n",
        "        continue\n",
        "    \n",
        "    # Split 80/20\n",
        "    split = int(len(X) * 0.8)\n",
        "    X_train, y_train = X.iloc[:split], y.iloc[:split]\n",
        "    X_test, y_test = X.iloc[split:], y.iloc[split:]\n",
        "    \n",
        "    try:\n",
        "        # Train model\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=5,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=4,\n",
        "            random_state=42\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluate\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "        test_r2 = r2_score(y_test, y_pred_test)\n",
        "        test_mape = np.mean(np.abs((y_test - y_pred_test) / (y_test + 1))) * 100\n",
        "        test_accuracy = max(0, 100 - test_mape)\n",
        "        \n",
        "        # Save model\n",
        "        safe_name = product.lower().replace(' ', '_').replace('/', '_')\n",
        "        model_file = f\"model_{safe_name}.joblib\"\n",
        "        joblib.dump(model, os.path.join('trained_models', model_file))\n",
        "        \n",
        "        trained_list.append(product)\n",
        "        metrics_summary.append({\n",
        "            'product': product,\n",
        "            'accuracy': round(test_accuracy, 2),\n",
        "            'r2': round(test_r2, 4),\n",
        "            'mae': round(test_mae, 2),\n",
        "            'rmse': round(test_rmse, 2)\n",
        "        })\n",
        "        \n",
        "        print(f\"[{i}/{len(top_products)}] ‚úì {product}\")\n",
        "        print(f\"           Accuracy: {test_accuracy:.2f}% | R¬≤: {test_r2:.4f} | MAE: {test_mae:.2f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[{i}/{len(top_products)}] ‚úó {product} - Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"‚úì Training complete! Successfully trained {len(trained_list)} models.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "train_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: View Accuracy Summary"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_summary)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal models trained: {len(metrics_df)}\")\n",
        "print(f\"Average Accuracy: {metrics_df['accuracy'].mean():.2f}%\")\n",
        "print(f\"Average R¬≤ Score: {metrics_df['r2'].mean():.4f}\")\n",
        "print(f\"Average MAE: {metrics_df['mae'].mean():.2f}\")\n",
        "print(f\"Average RMSE: {metrics_df['rmse'].mean():.2f}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "display(metrics_df.sort_values('accuracy', ascending=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax[0].hist(metrics_df['accuracy'], bins=15, color='skyblue', edgecolor='black')\n",
        "ax[0].axvline(metrics_df['accuracy'].mean(), color='red', linestyle='--', linewidth=2, \n",
        "              label=f\"Mean: {metrics_df['accuracy'].mean():.2f}%\")\n",
        "ax[0].set_xlabel('Accuracy (%)')\n",
        "ax[0].set_ylabel('Number of Models')\n",
        "ax[0].set_title('Model Accuracy Distribution', fontweight='bold')\n",
        "ax[0].legend()\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "ax[1].hist(metrics_df['r2'], bins=15, color='lightgreen', edgecolor='black')\n",
        "ax[1].axvline(metrics_df['r2'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "              label=f\"Mean: {metrics_df['r2'].mean():.4f}\")\n",
        "ax[1].set_xlabel('R¬≤ Score')\n",
        "ax[1].set_ylabel('Number of Models')\n",
        "ax[1].set_title('R¬≤ Score Distribution', fontweight='bold')\n",
        "ax[1].legend()\n",
        "ax[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save metrics\n",
        "with open('trained_models/model_metrics.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'summary': {\n",
        "            'total_models': len(metrics_df),\n",
        "            'avg_accuracy': float(metrics_df['accuracy'].mean()),\n",
        "            'avg_r2': float(metrics_df['r2'].mean()),\n",
        "            'avg_mae': float(metrics_df['mae'].mean())\n",
        "        },\n",
        "        'models': metrics_df.to_dict('records')\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úì Metrics saved to model_metrics.json\")"
      ],
      "metadata": {
        "id": "summary_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 7: Save Trained Products List"
      ],
      "metadata": {
        "id": "save"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained products list\n",
        "with open('trained_models/trained_articles.json', 'w') as f:\n",
        "    json.dump(trained_list, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Saved {len(trained_list)} product names to trained_articles.json\")\n",
        "print(\"\\nTrained products:\")\n",
        "for i, product in enumerate(trained_list, 1):\n",
        "    print(f\"  {i}. {product}\")"
      ],
      "metadata": {
        "id": "save_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 8: Download Models\n",
        "\n",
        "This will create a ZIP file with all trained models and download it."
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create ZIP\n",
        "shutil.make_archive('dejabrew_trained_models', 'zip', 'trained_models')\n",
        "\n",
        "print(\"‚úì Created dejabrew_trained_models.zip\")\n",
        "print(\"\\nZIP contents:\")\n",
        "!unzip -l dejabrew_trained_models.zip | head -20\n",
        "\n",
        "# Download\n",
        "print(\"\\nüì• Downloading...\")\n",
        "files.download('dejabrew_trained_models.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ SUCCESS! Training complete!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Extract dejabrew_trained_models.zip\")\n",
        "print(\"2. Copy all .joblib files to: dejabrew/forecasting/forecasting_data/\")\n",
        "print(\"3. Copy trained_articles.json to: dejabrew/forecasting/forecasting_data/\")\n",
        "print(\"4. Copy model_metrics.json to: dejabrew/forecasting/forecasting_data/ (optional)\")\n",
        "print(\"5. Restart your Django server\")\n",
        "print(\"6. Test with: python test_forecasting.py\")\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"  Average Accuracy: {metrics_df['accuracy'].mean():.2f}%\")\n",
        "print(f\"  Average R¬≤ Score: {metrics_df['r2'].mean():.4f}\")\n",
        "print(f\"  Total Models: {len(trained_list)}\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
