{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ DejaBrew Sales & Inventory Forecasting with Gradient Boosting\n",
        "\n",
        "This notebook trains Gradient Boosting Regression models on the **Coffee Shop Sales by Ahmed Abas** Kaggle dataset.\n",
        "\n",
        "**Dataset**: Coffee shop sales transaction records for Maven Roasters\n",
        "\n",
        "**Model**: Gradient Boosting Regressor with advanced feature engineering\n",
        "\n",
        "**Output**: Trained models (.joblib) with accuracy metrics for integration into DejaBrew system"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle scikit-learn pandas numpy joblib matplotlib seaborn -q"
      ],
      "metadata": {
        "id": "install_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Step 2: Setup Kaggle API\n",
        "\n",
        "1. Go to https://www.kaggle.com/settings/account\n",
        "2. Click \"Create New API Token\"\n",
        "3. Upload the `kaggle.json` file below"
      ],
      "metadata": {
        "id": "kaggle_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload your kaggle.json\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Setup Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"‚úì Kaggle API configured!\")"
      ],
      "metadata": {
        "id": "kaggle_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Step 3: Download Kaggle Dataset\n",
        "\n",
        "**Dataset**: Coffee Shop Sales by Ahmed Abas"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download the Coffee Shop Sales dataset\n",
        "!kaggle datasets download -d ahmedabbas757/coffee-sales\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q coffee-sales.zip -d coffee_data\n",
        "\n",
        "print(\"\\n‚úì Dataset downloaded and extracted!\")\n",
        "print(\"\\nFiles in dataset:\")\n",
        "!ls -lh coffee_data/"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Step 4: Load and Explore Data"
      ],
      "metadata": {
        "id": "explore"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Load the dataset (update filename if different)\n",
        "# Common names: 'Coffee Shop Sales.xlsx', 'index.xlsx', 'coffee_shop_sales.csv'\n",
        "# Let's check what file exists\n",
        "import glob\n",
        "files_in_data = glob.glob('coffee_data/*')\n",
        "print(\"Available files:\")\n",
        "for f in files_in_data:\n",
        "    print(f\" - {f}\")\n",
        "\n",
        "# Try to load the main file (adjust filename as needed)\n",
        "data_file = files_in_data[0] if files_in_data else 'coffee_data/Coffee Shop Sales.xlsx'\n",
        "\n",
        "# Load based on file extension\n",
        "if data_file.endswith('.xlsx') or data_file.endswith('.xls'):\n",
        "    df_raw = pd.read_excel(data_file)\n",
        "elif data_file.endswith('.csv'):\n",
        "    df_raw = pd.read_csv(data_file)\n",
        "else:\n",
        "    print(f\"Unknown file format: {data_file}\")\n",
        "    print(\"Please manually specify the correct file path below.\")\n",
        "    # df_raw = pd.read_excel('coffee_data/YOUR_FILE_NAME.xlsx')\n",
        "\n",
        "print(f\"\\n‚úì Loaded dataset: {data_file}\")\n",
        "print(f\"Shape: {df_raw.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_raw.head()"
      ],
      "metadata": {
        "id": "explore_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore data structure\n",
        "print(\"Dataset Info:\")\n",
        "print(df_raw.info())\n",
        "print(\"\\nColumn Names:\")\n",
        "print(df_raw.columns.tolist())\n",
        "print(\"\\nBasic Statistics:\")\n",
        "df_raw.describe()"
      ],
      "metadata": {
        "id": "explore_code2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 5: Data Preprocessing\n",
        "\n",
        "Convert the dataset to the format needed for training:\n",
        "- **date**: Transaction date\n",
        "- **product**: Product name (coffee/bakery item)\n",
        "- **quantity**: Number of items sold"
      ],
      "metadata": {
        "id": "preprocess"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust column names based on actual dataset structure\n",
        "# Common column names in Coffee Shop Sales dataset:\n",
        "# - 'transaction_date' or 'date' or 'Date'\n",
        "# - 'product_name' or 'product' or 'Product'\n",
        "# - 'quantity' or 'Quantity' or 'transaction_qty'\n",
        "\n",
        "# Let's create a mapping (UPDATE THESE based on your actual columns)\n",
        "# Example mappings:\n",
        "column_mapping = {\n",
        "    'transaction_date': 'date',  # UPDATE: Use actual date column name\n",
        "    'product_detail': 'product',  # UPDATE: Use actual product column name\n",
        "    'transaction_qty': 'quantity'  # UPDATE: Use actual quantity column name\n",
        "}\n",
        "\n",
        "# Alternative: If you see the columns above, update this mapping:\n",
        "# For example, if columns are 'Date', 'Product', 'Qty':\n",
        "# column_mapping = {'Date': 'date', 'Product': 'product', 'Qty': 'quantity'}\n",
        "\n",
        "print(\"Current columns:\", df_raw.columns.tolist())\n",
        "print(\"\\nPlease update column_mapping above if needed, then run again.\")\n",
        "\n",
        "# Apply mapping\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Try to rename columns\n",
        "try:\n",
        "    df = df.rename(columns=column_mapping)\n",
        "    \n",
        "    # Select only needed columns\n",
        "    df = df[['date', 'product', 'quantity']].copy()\n",
        "    \n",
        "    # Convert date to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    \n",
        "    # Remove invalid quantities\n",
        "    df = df[df['quantity'] > 0]\n",
        "    \n",
        "    # Strip whitespace from product names\n",
        "    df['product'] = df['product'].str.strip()\n",
        "    \n",
        "    print(f\"\\n‚úì Data preprocessed successfully!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    print(f\"Unique products: {df['product'].nunique()}\")\n",
        "    \n",
        "    print(\"\\nSample data:\")\n",
        "    display(df.head())\n",
        "    \nexcept Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"\\nPlease update the column_mapping dictionary above with correct column names.\")"
      ],
      "metadata": {
        "id": "preprocess_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate daily sales per product\n",
        "df_daily = df.groupby(['date', 'product'])['quantity'].sum().reset_index()\n",
        "\n",
        "print(f\"Daily aggregated data shape: {df_daily.shape}\")\n",
        "print(f\"\\nTop 10 products by total sales:\")\n",
        "top_products = df_daily.groupby('product')['quantity'].sum().sort_values(ascending=False).head(10)\n",
        "print(top_products)"
      ],
      "metadata": {
        "id": "aggregate_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: Visualize Sales Patterns"
      ],
      "metadata": {
        "id": "visualize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot top products\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10_products = df_daily.groupby('product')['quantity'].sum().sort_values(ascending=False).head(10)\n",
        "top_10_products.plot(kind='bar', color='steelblue')\n",
        "plt.title('Top 10 Products by Total Sales', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Product')\n",
        "plt.ylabel('Total Quantity Sold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot sales over time for top product\n",
        "top_product = top_10_products.index[0]\n",
        "plt.figure(figsize=(14, 5))\n",
        "product_data = df_daily[df_daily['product'] == top_product].set_index('date')['quantity']\n",
        "product_data.plot(color='darkgreen', linewidth=2)\n",
        "plt.title(f'Sales Trend for {top_product}', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Quantity Sold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Step 7: Feature Engineering\n",
        "\n",
        "Create advanced features for better forecasting:\n",
        "1. **Date features**: day_of_week, month, day_of_year, year\n",
        "2. **Lag features**: Sales from 1, 2, 3, 7, 14 days ago\n",
        "3. **Rolling averages**: 7-day and 14-day moving averages"
      ],
      "metadata": {
        "id": "features"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features_for_product(df_product):\n",
        "    \"\"\"\n",
        "    Creates advanced features for a single product's time series.\n",
        "    \n",
        "    Features include:\n",
        "    - Date features: day_of_week, month, day_of_year, year\n",
        "    - Lag features: quantity from 1, 2, 3, 7, 14 days ago\n",
        "    - Rolling averages: 7-day and 14-day windows\n",
        "    \"\"\"\n",
        "    # Ensure data is sorted by date\n",
        "    df_product = df_product.sort_values('date').copy()\n",
        "    df_product.set_index('date', inplace=True)\n",
        "    \n",
        "    # Create a continuous date range (fill missing dates with 0)\n",
        "    date_range = pd.date_range(start=df_product.index.min(), \n",
        "                                end=df_product.index.max(), \n",
        "                                freq='D')\n",
        "    df_product = df_product.reindex(date_range, fill_value=0)\n",
        "    \n",
        "    # Create feature dataframe\n",
        "    features = pd.DataFrame(index=df_product.index)\n",
        "    features['quantity'] = df_product['quantity']\n",
        "    \n",
        "    # 1. Date features\n",
        "    features['day_of_week'] = features.index.dayofweek  # 0=Monday, 6=Sunday\n",
        "    features['month'] = features.index.month\n",
        "    features['day_of_year'] = features.index.dayofyear\n",
        "    features['year'] = features.index.year\n",
        "    \n",
        "    # 2. Lag features (sales from previous days)\n",
        "    for lag in [1, 2, 3, 7, 14]:\n",
        "        features[f'lag_{lag}'] = features['quantity'].shift(lag)\n",
        "    \n",
        "    # 3. Rolling averages\n",
        "    features['rolling_mean_7'] = features['quantity'].rolling(window=7, min_periods=1).mean()\n",
        "    features['rolling_mean_14'] = features['quantity'].rolling(window=14, min_periods=1).mean()\n",
        "    \n",
        "    # Drop rows with NaN values (from lag features)\n",
        "    features = features.dropna()\n",
        "    \n",
        "    return features\n",
        "\n",
        "print(\"‚úì Feature engineering function created!\")\n",
        "print(\"\\nExample features for top product:\")\n",
        "\n",
        "# Test on top product\n",
        "top_product_data = df_daily[df_daily['product'] == top_product][['date', 'quantity']].copy()\n",
        "example_features = create_features_for_product(top_product_data)\n",
        "print(f\"\\nFeature shape: {example_features.shape}\")\n",
        "print(f\"Feature columns: {example_features.columns.tolist()}\")\n",
        "display(example_features.tail())"
      ],
      "metadata": {
        "id": "features_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Step 8: Train Gradient Boosting Models\n",
        "\n",
        "Train individual models for each product with accuracy evaluation."
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('trained_models', exist_ok=True)\n",
        "\n",
        "def train_model_for_product(product_name, df_product_data, min_data_points=30):\n",
        "    \"\"\"\n",
        "    Trains a Gradient Boosting model for a single product.\n",
        "    \n",
        "    Returns:\n",
        "        dict with model, metrics, and status\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create features\n",
        "        features = create_features_for_product(df_product_data)\n",
        "        \n",
        "        if len(features) < min_data_points:\n",
        "            return {\n",
        "                'status': 'skipped',\n",
        "                'reason': f'Insufficient data ({len(features)} points)'\n",
        "            }\n",
        "        \n",
        "        # Separate features and target\n",
        "        X = features.drop('quantity', axis=1)\n",
        "        y = features['quantity']\n",
        "        \n",
        "        # Split: 80% train, 20% test (chronological split)\n",
        "        split_idx = int(len(X) * 0.8)\n",
        "        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "        \n",
        "        # Train Gradient Boosting model\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=5,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=4,\n",
        "            random_state=42,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        # Training metrics\n",
        "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "        train_r2 = r2_score(y_train, y_pred_train)\n",
        "        train_mape = np.mean(np.abs((y_train - y_pred_train) / (y_train + 1))) * 100  # +1 to avoid division by zero\n",
        "        \n",
        "        # Test metrics\n",
        "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "        test_r2 = r2_score(y_test, y_pred_test)\n",
        "        test_mape = np.mean(np.abs((y_test - y_pred_test) / (y_test + 1))) * 100\n",
        "        \n",
        "        # Calculate accuracy percentage (100% - MAPE)\n",
        "        train_accuracy = max(0, 100 - train_mape)\n",
        "        test_accuracy = max(0, 100 - test_mape)\n",
        "        \n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'model': model,\n",
        "            'train_metrics': {\n",
        "                'mae': round(train_mae, 2),\n",
        "                'rmse': round(train_rmse, 2),\n",
        "                'r2': round(train_r2, 4),\n",
        "                'mape': round(train_mape, 2),\n",
        "                'accuracy': round(train_accuracy, 2)\n",
        "            },\n",
        "            'test_metrics': {\n",
        "                'mae': round(test_mae, 2),\n",
        "                'rmse': round(test_rmse, 2),\n",
        "                'r2': round(test_r2, 4),\n",
        "                'mape': round(test_mape, 2),\n",
        "                'accuracy': round(test_accuracy, 2)\n",
        "            },\n",
        "            'data_points': len(features),\n",
        "            'train_size': len(X_train),\n",
        "            'test_size': len(X_test)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'reason': str(e)\n",
        "        }\n",
        "\n",
        "print(\"‚úì Model training function created!\")"
      ],
      "metadata": {
        "id": "train_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models for top products\n",
        "# You can adjust this number based on your needs\n",
        "TOP_N_PRODUCTS = 30  # Train models for top 30 products\n",
        "\n",
        "# Get top products by total sales\n",
        "product_sales = df_daily.groupby('product')['quantity'].sum().sort_values(ascending=False)\n",
        "top_products_list = product_sales.head(TOP_N_PRODUCTS).index.tolist()\n",
        "\n",
        "print(f\"Training models for top {TOP_N_PRODUCTS} products...\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Store results\n",
        "training_results = {}\n",
        "trained_models = {}\n",
        "metrics_summary = []\n",
        "\n",
        "for i, product in enumerate(top_products_list, 1):\n",
        "    print(f\"\\n[{i}/{len(top_products_list)}] Training: {product}\")\n",
        "    \n",
        "    # Get product data\n",
        "    product_data = df_daily[df_daily['product'] == product][['date', 'quantity']].copy()\n",
        "    \n",
        "    # Train model\n",
        "    result = train_model_for_product(product, product_data)\n",
        "    \n",
        "    if result['status'] == 'success':\n",
        "        print(f\"  ‚úì Success!\")\n",
        "        print(f\"  Data points: {result['data_points']} (Train: {result['train_size']}, Test: {result['test_size']})\")\n",
        "        print(f\"  Test Accuracy: {result['test_metrics']['accuracy']}%\")\n",
        "        print(f\"  Test R¬≤: {result['test_metrics']['r2']}\")\n",
        "        print(f\"  Test MAE: {result['test_metrics']['mae']}\")\n",
        "        print(f\"  Test RMSE: {result['test_metrics']['rmse']}\")\n",
        "        \n",
        "        # Save model\n",
        "        safe_name = product.lower().replace(' ', '_').replace('/', '_')\n",
        "        model_filename = f\"model_{safe_name}.joblib\"\n",
        "        model_path = os.path.join('trained_models', model_filename)\n",
        "        joblib.dump(result['model'], model_path)\n",
        "        \n",
        "        trained_models[product] = model_path\n",
        "        training_results[product] = result\n",
        "        \n",
        "        # Add to summary\n",
        "        metrics_summary.append({\n",
        "            'product': product,\n",
        "            'test_accuracy': result['test_metrics']['accuracy'],\n",
        "            'test_r2': result['test_metrics']['r2'],\n",
        "            'test_mae': result['test_metrics']['mae'],\n",
        "            'data_points': result['data_points']\n",
        "        })\n",
        "        \n",
        "    elif result['status'] == 'skipped':\n",
        "        print(f\"  ‚äò Skipped: {result['reason']}\")\n",
        "    else:\n",
        "        print(f\"  ‚úó Error: {result['reason']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"\\n‚úì Training complete! Successfully trained {len(trained_models)} models.\")"
      ],
      "metadata": {
        "id": "train_loop_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Step 9: Accuracy Report"
      ],
      "metadata": {
        "id": "accuracy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create accuracy summary DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_summary)\n",
        "metrics_df = metrics_df.sort_values('test_accuracy', ascending=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL ACCURACY SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal models trained: {len(metrics_df)}\")\n",
        "print(f\"Average Test Accuracy: {metrics_df['test_accuracy'].mean():.2f}%\")\n",
        "print(f\"Average Test R¬≤: {metrics_df['test_r2'].mean():.4f}\")\n",
        "print(f\"Average Test MAE: {metrics_df['test_mae'].mean():.2f}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nDetailed Metrics by Product:\")\n",
        "print(\"=\"*80)\n",
        "display(metrics_df)\n",
        "\n",
        "# Visualize accuracy distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy distribution\n",
        "axes[0].hist(metrics_df['test_accuracy'], bins=20, color='skyblue', edgecolor='black')\n",
        "axes[0].axvline(metrics_df['test_accuracy'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {metrics_df[\"test_accuracy\"].mean():.2f}%')\n",
        "axes[0].set_xlabel('Test Accuracy (%)')\n",
        "axes[0].set_ylabel('Number of Models')\n",
        "axes[0].set_title('Distribution of Model Accuracy', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# R¬≤ Score distribution\n",
        "axes[1].hist(metrics_df['test_r2'], bins=20, color='lightgreen', edgecolor='black')\n",
        "axes[1].axvline(metrics_df['test_r2'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {metrics_df[\"test_r2\"].mean():.4f}')\n",
        "axes[1].set_xlabel('Test R¬≤ Score')\n",
        "axes[1].set_ylabel('Number of Models')\n",
        "axes[1].set_title('Distribution of R¬≤ Scores', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save metrics to JSON\n",
        "metrics_dict = metrics_df.to_dict('records')\n",
        "with open('trained_models/model_metrics.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'summary': {\n",
        "            'total_models': len(metrics_df),\n",
        "            'avg_test_accuracy': float(metrics_df['test_accuracy'].mean()),\n",
        "            'avg_test_r2': float(metrics_df['test_r2'].mean()),\n",
        "            'avg_test_mae': float(metrics_df['test_mae'].mean())\n",
        "        },\n",
        "        'models': metrics_dict\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úì Metrics saved to trained_models/model_metrics.json\")"
      ],
      "metadata": {
        "id": "accuracy_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 10: Save Model Metadata"
      ],
      "metadata": {
        "id": "save_metadata"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save list of trained products\n",
        "trained_products = list(trained_models.keys())\n",
        "with open('trained_models/trained_articles.json', 'w') as f:\n",
        "    json.dump(trained_products, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Saved {len(trained_products)} product names to trained_articles.json\")\n",
        "print(\"\\nTrained products:\")\n",
        "for i, product in enumerate(trained_products, 1):\n",
        "    print(f\"  {i}. {product}\")"
      ],
      "metadata": {
        "id": "save_metadata_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 11: Download Trained Models\n",
        "\n",
        "Download all trained models as a ZIP file for integration into your DejaBrew system."
      ],
      "metadata": {
        "id": "download_models"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create ZIP file\n",
        "shutil.make_archive('dejabrew_trained_models', 'zip', 'trained_models')\n",
        "\n",
        "print(\"‚úì Created dejabrew_trained_models.zip\")\n",
        "print(\"\\nContents:\")\n",
        "!unzip -l dejabrew_trained_models.zip | head -20\n",
        "\n",
        "# Download the ZIP file\n",
        "print(\"\\nDownloading...\")\n",
        "files.download('dejabrew_trained_models.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úì TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Extract dejabrew_trained_models.zip\")\n",
        "print(\"2. Copy all .joblib files to: dejabrew/forecasting/forecasting_data/\")\n",
        "print(\"3. Copy trained_articles.json to: dejabrew/forecasting/forecasting_data/\")\n",
        "print(\"4. Restart your Django server\")\n",
        "print(\"5. Your forecasting system will now use the trained models!\")\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"  - Average Accuracy: {metrics_df['test_accuracy'].mean():.2f}%\")\n",
        "print(f\"  - Average R¬≤ Score: {metrics_df['test_r2'].mean():.4f}\")\n",
        "print(f\"  - Total Models: {len(trained_models)}\")"
      ],
      "metadata": {
        "id": "download_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Step 12: Test Predictions (Optional)\n",
        "\n",
        "Test the trained model with sample predictions."
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction for a product\n",
        "test_product = trained_products[0] if trained_products else None\n",
        "\n",
        "if test_product:\n",
        "    print(f\"Testing predictions for: {test_product}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Load model\n",
        "    model = joblib.load(trained_models[test_product])\n",
        "    \n",
        "    # Get historical data\n",
        "    product_data = df_daily[df_daily['product'] == test_product][['date', 'quantity']].copy()\n",
        "    features = create_features_for_product(product_data)\n",
        "    \n",
        "    # Get last 14 days for context\n",
        "    last_14_days = features.tail(14).copy()\n",
        "    X_last = last_14_days.drop('quantity', axis=1)\n",
        "    y_actual = last_14_days['quantity']\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X_last)\n",
        "    \n",
        "    # Create comparison\n",
        "    comparison = pd.DataFrame({\n",
        "        'date': X_last.index,\n",
        "        'actual': y_actual.values,\n",
        "        'predicted': y_pred.round(0).astype(int)\n",
        "    })\n",
        "    \n",
        "    print(\"\\nLast 14 Days - Actual vs Predicted:\")\n",
        "    display(comparison)\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(comparison['date'], comparison['actual'], marker='o', label='Actual', linewidth=2)\n",
        "    plt.plot(comparison['date'], comparison['predicted'], marker='s', label='Predicted', linewidth=2, linestyle='--')\n",
        "    plt.title(f'Actual vs Predicted Sales for {test_product}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Quantity')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate test accuracy\n",
        "    test_mae = mean_absolute_error(y_actual, y_pred)\n",
        "    test_mape = np.mean(np.abs((y_actual - y_pred) / (y_actual + 1))) * 100\n",
        "    test_accuracy = max(0, 100 - test_mape)\n",
        "    \n",
        "    print(f\"\\nPrediction Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"Mean Absolute Error: {test_mae:.2f}\")"
      ],
      "metadata": {
        "id": "test_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
